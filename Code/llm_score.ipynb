{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the Steps:\n",
    "\n",
    "\t1.\tLoad and Preprocess the Data:\n",
    "\t•\tRead the dataset from your .xlsx files.\n",
    "\t•\tCount the number of comments per user.\n",
    "\t•\tGroup users based on the number of comments they have.\n",
    "\t•\tRandomly sample users from each group.\n",
    "\t2.\tAnalyze Comments with the LLM:\n",
    "\t•\tFor each sampled user, collect their comments.\n",
    "\t•\tUse OpenAI’s GPT-4o-mini model to analyze the text and assign a happiness score to each comment.\n",
    "\t•\tEnsure consistent scoring methodology across all users.\n",
    "\t3.\tAggregate and Analyze Results:\n",
    "\t•\tCompute average happiness scores per user.\n",
    "\t•\tCompare these scores with the users’ subjective well-being (SWB) scores from the survey.\n",
    "\t•\tPerform statistical analyses to explore correlations and validate your methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai (from -r requirements.txt (line 1))\n",
      "  Using cached openai-1.54.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting python-dotenv (from -r requirements.txt (line 2))\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting requests (from -r requirements.txt (line 3))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "\t•\tData Loading: We use pandas to read the Excel file and ensure all required columns are present.\n",
    "\t•\tCounting Comments: We group the data by users and count the number of comments per user.\n",
    "\t•\tGrouping Users: Users are categorized into groups based on their comment counts.\n",
    "\t•\tSampling Users: We randomly sample an equal number of users from each group to ensure balanced representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Load your OpenAI API key from the .env file\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Step 1: Load and Preprocess the Data\n",
    "\n",
    "# Replace 'data.xlsx' with the path to your dataset file\n",
    "data = pd.read_excel('test.xlsx')\n",
    "\n",
    "# Ensure that all necessary columns are present\n",
    "required_columns = ['a', 'Type', 'ContentTimestamp', 'Content', 'Upvotes',\n",
    "                    'ContentSubreddit', 'Score', 'QuestionID', 'SurveyResponse', 'SurveyTimestamp']\n",
    "\n",
    "if not all(column in data.columns for column in required_columns):\n",
    "    raise ValueError(\"One or more required columns are missing in the dataset.\")\n",
    "\n",
    "# Count the number of comments per user\n",
    "user_comment_counts = data.groupby('a')['Content'].count().reset_index()\n",
    "user_comment_counts.columns = ['user', 'comment_count']\n",
    "\n",
    "# Define user groups based on comment counts\n",
    "def assign_group(count):\n",
    "    if count < 25:\n",
    "        return 'Under 25'\n",
    "    elif count < 100:\n",
    "        return '25-99'\n",
    "    elif count < 1000:\n",
    "        return '100-999'\n",
    "    else:\n",
    "        return '1000+'\n",
    "\n",
    "user_comment_counts['group'] = user_comment_counts['comment_count'].apply(assign_group)\n",
    "\n",
    "# Sample users from each group\n",
    "sampled_users = []\n",
    "group_sizes = {'Under 25': 10, '25-99': 10, '100-999': 10, '1000+': 10}\n",
    "\n",
    "for group, size in group_sizes.items():\n",
    "    users_in_group = user_comment_counts[user_comment_counts['group'] == group]\n",
    "    sample_size = min(size, len(users_in_group))\n",
    "    sampled = users_in_group.sample(n=sample_size, random_state=42)\n",
    "    sampled_users.extend(sampled['user'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "\t•\tFiltering Data: We select only the data corresponding to the sampled users.\n",
    "\t•\tLLM Function: The get_happiness_score function sends each comment to the LLM and retrieves a happiness score.\n",
    "\t•\tError Handling: If the LLM fails to provide a valid score, we assign NaN to handle missing data appropriately.\n",
    "\t•\tAPI Rate Limiting: We add a delay after each API call to respect rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for the sampled users\n",
    "sampled_data = data[data['a'].isin(sampled_users)].copy()\n",
    "\n",
    "# Function to get happiness score from the LLM\n",
    "def get_happiness_score(text):\n",
    "    # Prepare the prompt\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that rates the happiness expressed in a given text on a scale from 1 to 10, where 1 is very unhappy and 10 is very happy. Only provide the numerical score.\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            max_tokens=5,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0\n",
    "        )\n",
    "        # Extract the score from the response\n",
    "        score_str = response['choices'][0]['message']['content'].strip()\n",
    "        # Convert to float\n",
    "        score = float(score_str)\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return np.nan  # Use NaN for missing values\n",
    "\n",
    "# Apply the function to the comments\n",
    "# Since API calls can be rate-limited, we should process with care\n",
    "happiness_scores = []\n",
    "for idx, row in sampled_data.iterrows():\n",
    "    text = row['Content']\n",
    "    score = get_happiness_score(text)\n",
    "    happiness_scores.append(score)\n",
    "    # To avoid hitting the rate limit, add a delay\n",
    "    time.sleep(1)  # Adjust based on your API rate limits\n",
    "\n",
    "sampled_data['happiness_score'] = happiness_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "\t•\tAggregating Scores: We compute the average happiness score per user from the LLM outputs.\n",
    "\t•\tSWB Scores: We extract and average the SWB scores for each user from the survey responses.\n",
    "\t•\tMerging Data: We combine the LLM happiness scores and SWB scores into a single DataFrame for analysis.\n",
    "\t•\tStatistical Analysis: We calculate the Pearson correlation coefficient to assess the relationship between the LLM scores and SWB scores.\n",
    "\t•\tSaving Results: Optionally, we save the merged data to a CSV file for further analysis or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average happiness scores per user\n",
    "user_happiness = sampled_data.groupby('a')['happiness_score'].mean().reset_index()\n",
    "user_happiness.columns = ['user', 'average_happiness_score']\n",
    "\n",
    "# Get SWB scores for the sampled users\n",
    "swb_data = data[data['a'].isin(sampled_users) & data['QuestionID'].isin(['Q1', 'Q2'])]\n",
    "user_swb = swb_data.groupby(['a', 'QuestionID'])['Score'].mean().reset_index()\n",
    "user_swb = user_swb.pivot(index='a', columns='QuestionID', values='Score').reset_index()\n",
    "user_swb.columns = ['user', 'SWB_Q1', 'SWB_Q2']\n",
    "\n",
    "# Merge happiness scores with SWB scores\n",
    "merged_data = pd.merge(user_happiness, user_swb, on='user')\n",
    "\n",
    "# Perform statistical analysis\n",
    "# For example, compute Pearson correlation between average happiness score and SWB scores\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Correlation with SWB_Q1\n",
    "corr_q1, p_value_q1 = pearsonr(merged_data['average_happiness_score'], merged_data['SWB_Q1'])\n",
    "print(f\"Correlation between LLM happiness scores and SWB_Q1: {corr_q1}, p-value: {p_value_q1}\")\n",
    "\n",
    "# Correlation with SWB_Q2\n",
    "corr_q2, p_value_q2 = pearsonr(merged_data['average_happiness_score'], merged_data['SWB_Q2'])\n",
    "print(f\"Correlation between LLM happiness scores and SWB_Q2: {corr_q2}, p-value: {p_value_q2}\")\n",
    "\n",
    "# Optional: Save the merged data for further analysis\n",
    "merged_data.to_csv('merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjustments and Considerations:\n",
    "\n",
    "\t•\tAPI Rate Limits and Costs:\n",
    "\t•\tBe mindful of the OpenAI API’s rate limits and potential costs, especially with a large dataset.\n",
    "\t•\tConsider batching requests or using OpenAI’s asynchronous API features if necessary.\n",
    "\t•\tError Handling:\n",
    "\t•\tThe get_happiness_score function includes basic error handling.\n",
    "\t•\tIf the LLM fails to provide a valid score, we assign NaN to handle missing data appropriately in analysis.\n",
    "\t•\tScalability:\n",
    "\t•\tSince processing every comment might be impractical, sampling users and their comments helps manage the workload.\n",
    "\t•\tYou might further limit the number of comments per user if needed.\n",
    "\t•\tConsistency in Scoring:\n",
    "\t•\tUsing a fixed prompt and setting temperature=0 ensures deterministic outputs from the LLM, enhancing consistency.\n",
    "\t•\tStatistical Analysis:\n",
    "\t•\tBeyond Pearson correlation, you might explore other statistical tests or models (e.g., regression analysis) to deepen your insights.\n",
    "\t•\tVisualizations (e.g., scatter plots) can also be helpful.\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "\t•\tValidation:\n",
    "\t•\tReview a subset of the LLM’s outputs to ensure that the scores make sense and the model is interpreting the text as expected.\n",
    "\t•\tDocumentation:\n",
    "\t•\tKeep detailed records of your methodology, including any parameters or thresholds used, to maintain transparency and reproducibility.\n",
    "\t•\tEthical Considerations:\n",
    "\t•\tEnsure compliance with Reddit’s API terms and conditions and respect user privacy.\n",
    "\t•\tAnonymize data if necessary when sharing results.\n",
    "\n",
    "Notes on Adjusting Your Approach:\n",
    "\n",
    "Given the scale of your data and the potential costs and time associated with processing a large number of comments, grouping users and sampling is a practical approach. This method allows you to:\n",
    "\n",
    "\t•\tManage Resources: Limit the number of API calls to a feasible amount.\n",
    "\t•\tStatistical Validity: By sampling from different user groups, you maintain representation across the spectrum of user activity levels.\n",
    "\t•\tFocus on Quality: With a manageable dataset, you can spend more time ensuring the accuracy and reliability of your results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
